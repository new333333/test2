<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">

<!-- saved from url=(0014)about:internet -->
<html>
<head>

<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta http-equiv="Content-Language" content="en" />
<meta name="Generator" content="Novell Documentation" />
<meta name="Generation-Date" content="Monday, May 05, 2008 19:00" />
<meta name="Content-Date" content="Mon, 01 Jan 2003 12:00:00 GMT" />
<meta name="Copyright" content="Copyright Novell, Inc. 2006" />

<style type="text/css">
  @import url(ui/styles.css);
</style>

<title>Load-Balancing and Clustering Novell Teaming</title>
<script type="text/javascript">
  var prev_link = "bbn5klr.html";
  var next_link = "installer.html";
  var parentfile = "../b9vcafr.html";
  var parenttoc = "../toc_b9vcafr.html";
  if(parent.location != location) { parent.loadMenu(); }
</script>
<script type="text/javascript" src="ui/jsglobal.js"></script>
</head>

<body>
<script type="text/javascript" src="ui/topnav.js"></script>

<div class="sect1"  id="bbnqb4c">

<h1 class="title"> Load-Balancing and Clustering Novell Teaming</h1>

<div class="note">
<p class="para"><em class="title">NOTE:</em>In this release, Novell Teaming does not support session replication across cluster nodes. </p></div>

<div class="important">
<p class="para"><em class="title">IMPORTANT:</em>If you have more than one clustered installation of Novell Teaming in your network, each cluster group needs a unique multicast group IP address to prevent them from interfering with each other.</p></div>

<p class="para">To set up a scalable, clustered Novell Teaming configuration, use the following steps as a guideline:</p>
<div class="procedure">
<ol class="steps">
<li class="step" id="bbnqei7">
<p class="para">Set up a shared file storage that is accessible to all nodes.</p>
<p class="para">Novell Teaming’s Simple File Repository is safe for use in a clustered environment with shared storage that is accessible to all nodes. Setting up shared storage is a platform-specific task.</p></li>
<li class="step" id="bbnqewc">
<p class="para">Install and configure the Lucene Index Server.</p>
<p class="para">See <a href="bbijvt7.html">Installing a Standalone Lucene Index Server</a> for how to place the Lucene Index Server on a dedicated server system. All cluster nodes share this index server.</p></li>
<li class="step" id="bbnqewf">
<p class="para">Make sure that the time is the same on all nodes in the cluster (using synchronized time services for your cluster is highly advised).</p></li>
<li class="step" id="bbnqewh">
<p class="para">Install the Novell Teaming/portal bundle kit on each cluster node.</p>
<div class="important">
<p class="para"><em class="title">IMPORTANT:</em>Because all the nodes share the same database server, it is important not to execute the database-initialization SQL scripts more than once (that is, use the installer’s <span class="guimenu">Reconfigure</span> option rather than the <span class="guimenu">New installation</span> option on all but the first cluster node).</p></div>

<p class="para">Each node in the cluster can use the same<span class="literal"> installer.xml </span>file (generated by the installer during initial installation). This assures that cluster nodes are configured uniformly.</p>
<p class="para">The following settings must be uniform across the cluster:</p>
<div class="itemizedlist">
<ul class="listbullet">
<li class="listitem">
<p class="listitem">Configure the database connection settings on each node to use the same database. </p></li>
<li class="listitem">
<p class="listitem">Set the file system settings to all point to the same shared file storage server.</p></li>
<li class="listitem">
<p class="listitem">Set the Network settings (name, port, etc.) to the name of the LOAD BALANCER system.</p></li>
<li class="listitem">
<p class="listitem">In the <span class="guimenu">Lucene configuration</span> window of the installer, select <span class="guimenu">server</span> in the <span class="guimenu">Lucene configuration type</span> drop-down list and set the <span class="guimenu">Host</span> selection to the hostname of the machine on which you installed the Lucene Index Server. See <a href="bbijvt7.html">Installing a Standalone Lucene Index Server</a>.</p></li></ul></div>
</li>
<li class="step" id="bbo3q8t">
<p class="para">Set the portal up to work in a clustered environment.</p>
<p class="para">If you need to set more advanced configurations, use <span class="literal">cache.cluster.properties</span> instead of <span class="literal">cache.cluster.multicast.ip</span> in the steps below. For more information, refer to Liferay's documentation at <a href="http://wiki.liferay.com/index.php/Clustering" class="ulink">wiki.liferay.com</a> and <a href="http://wiki.liferay.com/index.php/Liferay_FAQ" class="ulink">http://wiki.liferay.com</a>. Liferay provides additional high availability information at <a href="http://wiki.liferay.com/index.php/High_Availability_Guide" class="ulink">wiki.liferay.com</a>. </p>
<div class="substeps">
<ol class="steps">
<li class="step" id="bdh0t3r">
<p class="para">Edit the<span class="literal"> /webapps/ROOT/WEB-INF/classes/portal-ext.properties </span>file by uncommenting the following two lines: </p><pre class="screen">
cache.event.listeners=com.opensymphony.oscache.plugins.clustersupport.JavaGroupsBroadcastingListener
</pre><pre class="screen">
cache.cluster.multicast.ip=231.12.21.100
</pre></li>
<li class="step" id="bdh0tyq">
<p class="para">Edit the /webapps/ROOT/WEB-INF/classes/cache-multi-vm-ext.properties file by uncommenting the following two lines:</p><pre class="screen">
cache.event.listeners=com.opensymphony.oscache.plugins.clustersupport.JavaGroupsBroadcastingListener
</pre><pre class="screen">
cache.cluster.multicast.ip=231.12.21.101
</pre></li></ol>
</div>
</li>
<li class="step" id="bbo3q8v">
<p class="para">Configure the load balancer.</p>
<p class="para">There are a variety of load balancing solutions that work with J2EE deployments. The following example configuration uses the balancer module built into the newer Apache (version 2.2.4), and is based on a widely used sticky session technique. Novell Teaming does not support session sharing/replication among Tomcat instances.</p>
<div class="substeps">
<ol class="steps">
<li class="step" id="bbo3xrk">
<p class="para">Edit Tomcat’s<span class="literal"> server.xml</span>.<span class="literal"></span></p>
<p class="para">Add<span class="literal"> jvmRoute="jvm&lt;n&gt;" </span>to the<span class="literal">&lt;Engine name="Catalina" ...&gt; </span>element, where<span class="literal">&lt;n&gt; </span>should be an integer unique to each Tomcat instance. For example, assuming you have two Tomcat instances in a cluster, the Catalina engine element should look like the following in each<span class="literal"> server.xml </span>respectively (<span class="literal">jvm&lt;n&gt; </span>is the name of the worker as declared in the load balancer):</p><pre class="screen">
&lt;Engine name="Catalina" defaultHost="localhost" jvmRoute="jvm1"&gt;
</pre><pre class="screen">
&lt;Engine name="Catalina" defaultHost="localhost" jvmRoute="jvm2"&gt;
</pre></li>
<li class="step" id="bbo4598">
<p class="para">Edit Apache’s<span class="literal"> httpd.conf file </span>(in the<span class="literal">&lt;apache installation&gt;/conf </span>directory).</p>
<div class="orderedlist">
<ol class="orderedlist">
                
<li class="listitem">
<p class="listitem">Uncomment the following three lines:</p><pre class="screen">
LoadModule proxy_module modules/mod_proxy.so
</pre><pre class="screen">
LoadModule proxy_ajp_module modules/mod_proxy_ajp.so
</pre><pre class="screen">
LoadModule proxy_balancer_module modules/mod_proxy_balancer.so
</pre></li>
                
<li class="listitem">
<p class="listitem">Append the following section to the end of the file:</p><pre class="screen">
&lt;Location /balancer-manager&gt;
</pre><pre class="screen">
SetHandler balancer-manager
</pre><pre class="screen">
Order deny,allow
</pre><pre class="screen">
Deny from all
</pre><pre class="screen">
Allow from 127.0.0.1
</pre><pre class="screen">
&lt;/Location&gt;
</pre><pre class="screen">
&lt;Proxy balancer://aspenCluster&gt;
</pre><pre class="screen">
BalancerMember ajp://&lt;tomcat host name 1&gt;:8009 route=jvm1
</pre><pre class="screen">
BalancerMember ajp://&lt;tomcat host name 2&gt;:8009 route=jvm2
</pre><pre class="screen">
&lt;/Proxy&gt;
</pre><pre class="screen">
&lt;Location /&gt;
</pre><pre class="screen">
ProxyPass balancer://aspenCluster/ stickysession=JSESSIONID    
</pre><pre class="screen">
&lt;/Location&gt;
</pre>
<p class="listitem">Substitute the real Tomcat hostnames for<span class="literal">&lt;tomcat host name 1&gt; </span>and<span class="literal">&lt;tomcat host name 2&gt;</span>. If you have more than two Tomcat instances in the cluster, make an additional line for each.</p></li>
              </ol></div>
</li></ol>
</div>
</li>
<li class="step" id="bbo3q8x">
<p class="para">Configure Hibernate second-level cache to use cluster-safe distributed cache. To do this, rename the<span class="literal"> ehcache-hibernate.xml </span>file in the<span class="literal"> webapps/ssf/WEB-INF/classes/config </span>directory to something else, say,<span class="literal"> ehcache-hibernate-non-clustered.xml</span>. Then, rename the<span class="literal"> ehcache-hibernate-clustered.xml </span>file in the same directory to<span class="literal"> ehcache-hibernate.xml</span>.</p></li>
<li class="step" id="bbo3q8y">
<p class="para">Start the Lucene Index Server, and then start the application cluster nodes.</p></li></ol>
</div>

</div>

<p class="trademark">A trademark symbol (<sup><small>&reg;</small></sup>, <sup><small>TM</small></sup>, etc.) denotes a Novell trademark. An asterisk (*) denotes a third-party trademark. For more information, see <a href="legal.html">Legal Notices</a>.</p>

<script type="text/javascript" src="ui/bottomnav.js"></script>
</body>
</html>